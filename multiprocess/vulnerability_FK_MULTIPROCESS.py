
import os

os.chdir('D:\\ENEA_CAS_WORK\\Catania_RAFAEL\\viasat_data')
os.getcwd()

import numpy as np
import pandas as pd
import geopandas as gpd
from geopandas import GeoDataFrame
from shapely.geometry import Point
import folium
import osmnx as ox
import networkx as nx
import math
import momepy
# from funcs_network_FK import roads_type_folium
from shapely import geometry
from shapely.geometry import Point, Polygon
import psycopg2
import db_connect
import datetime
import seaborn as sns
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from folium_stuff_FK_map_matching import plot_graph_folium_FK
import glob
from funcs_network_FK import cost_assignment
import statistics
from sqlalchemy import *
import sqlalchemy as sal
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
pd.options.mode.chained_assignment = None  # default='warn'

import multiprocessing as mp
from multiprocessing import Process, freeze_support, Manager
from time import sleep
from collections import deque
from multiprocessing.managers import BaseManager
import contextlib
from multiprocessing import Manager
from multiprocessing import Pool

import dill as Pickle
from joblib import Parallel, delayed
from joblib.externals.loky import set_loky_pickler
set_loky_pickler('pickle')
from multiprocessing import Pool,RLock


#####
##########
###################
##########################
###############################
#####################################
#############################################
#####################################################
#########################################################
###############################################################
###############################################################
### VULNERABILITY ALGORITHM ###################################
###############################################################
###############################################################

# connect to new DB to be populated with Viasat data after route-check
conn_HAIG = db_connect.connect_HAIG_Viasat_CT()
cur_HAIG = conn_HAIG.cursor()

# erase existing table
# cur_HAIG.execute("DROP TABLE IF EXISTS vulnerability_all_FEBRUARY_2019 CASCADE")
# conn_HAIG.commit()


### check the size of the WHOLE DB "HAIG_Viasat_CT"
# pd.read_sql_query('''
# SELECT pg_size_pretty( pg_database_size('HAIG_Viasat_CT') )''', conn_HAIG)

# ### check size of table "vulnerability_pesanti_2019"
# pd.read_sql_query('''
# SELECT pg_size_pretty( pg_relation_size('public.vulnerability_pesanti_2019') )''', conn_HAIG)
#
# pd.read_sql_query('''
# SELECT pg_size_pretty( pg_relation_size('public.vulnerability_2019') )''', conn_HAIG)


# Create an SQL connection engine to the output DB
# engine = sal.create_engine('postgresql://postgres:superuser@192.168.132.18:5432/HAIG_Viasat_CT')
engine = sal.create_engine('postgresql://postgres:superuser@10.0.0.1:5432/HAIG_Viasat_CT')
# engine = sal.create_engine('postgresql://postgres:vaxcrio1@192.168.134.43:5432/HAIG_Viasat_CT?gssencmode=disable')
# connection = engine.connect()


# load complete TIME_EDGES files obtained from VIASAT analysis
os.chdir('D:\\ENEA_CAS_WORK\\Catania_RAFAEL\\viasat_data')
# load all EDGES obtained from the map-matching algorithm

## sottorete referred to "veicoli pesanti" but with the "counts" from all vehicles (veicoli pesanti + automobili)
# TIME_EDGES = gpd.read_file("sottorete_speed.geojson")
# TIME_EDGES = gpd.read_file("sottorete_speed_counts_all_vehicles_FEBRUARY.geojson")
TIME_EDGES = gpd.read_file("sottorete_speed_counts_all_vehicles_AUGUST.geojson")
# TIME_EDGES = gpd.read_file("sottorete_speed_counts_all_vehicles.geojson")

## sottoret referred to "veicoli pesanti" but with the "counts" only from "veicoli pesanti"
# TIME_EDGES = gpd.read_file("sottorete_speed_counts_veicoli_pesanti.geojson")
# TIME_EDGES.plot()

# load all hexagonal cells (hex_grid)
# hex_grid = gpd.read_file("hex_grid.geojson")
# hex_grid = gpd.read_file("hex_grid_400m.geojson")
# hex_grid = gpd.read_file("hex_grid_400m_FEBRUARY.geojson")
hex_grid = gpd.read_file("hex_grid_400m_AUGUST.geojson")


### load grafo with cost obtained from VIASAT data (map-matching)
file_graphml = 'CATANIA_VIASAT_cost.graphml'
grafo = ox.load_graphml(file_graphml)
# make attr "cost" as float!!! (units of "cost" are SECONDS!)
for u, v, key, attr in grafo.edges(keys=True, data=True):
    if len(attr['VIASAT_cost']) > 0:
        attr['VIASAT_cost'] = float(attr.get("VIASAT_cost"))
    if len(attr['cost']) >0:
        attr['cost'] = float(attr.get("cost"))

# get geodatframes for nodes and edges
gdf_nodes, gdf_edges = ox.graph_to_gdfs(grafo)

# change directory
# os.chdir('D:\\ENEA_CAS_WORK\\Catania_RAFAEL\\postprocessing\\vulnerability')

####################################################################################
####################################################################################
# create basemap around Catania
ave_LAT = 37.53988692816245
ave_LON = 15.044971594798902
my_map = folium.Map([ave_LAT, ave_LON], zoom_start=8.5, tiles='cartodbpositron')
####################################################################################
####################################################################################

# # load all 'elements' (these are the indices of the hex cells associated to all edges crossing each of them)
# elements = np.load('elements.npy',allow_pickle='TRUE').item()
# elements = np.load('elements_400m_FEBRUARY.npy',allow_pickle='TRUE').item()
elements = np.load('elements_400m_AUGUST.npy',allow_pickle='TRUE').item()
# elements = np.load('elements_400m_Feb_May_Aug_Nov.npy',allow_pickle='TRUE').item()


###################################################################################
## calculate the initial travel timeS and shortest paths between the OD pairs #####
###################################################################################

penalty = 18000  # PENALTY time or DISRUPTION time (seconds of closure of the link (u,v))
## set 'importance' of an edges to 0 at the NULL scenario (no disruption)
TIME_EDGES['importance'] = 0
hex_grid['importance'] = 0

## load al ORIGINS and DESTINATIONS of the "sottorete"
# all_catania_OD = pd.read_csv("all_catania_OD.csv", delimiter=',')
# all_catania_OD = pd.read_csv("all_catania_OD_FEBRUARY.csv", delimiter=',')
all_catania_OD = pd.read_csv("all_catania_OD_AUGUST.csv", delimiter=',')
# all_catania_OD = pd.read_csv("all_catania_OD_AUGUST_300_vehicles.csv", delimiter=',')
# all_catania_OD = all_catania_OD.sample(n=100000)
# all_catania_OD = pd.read_csv("all_catania_OD_FEBRUARY_300_vehicles.csv", delimiter=',')
# all_catania_OD = all_catania_OD.sample(n=200000)
# all_catania_OD = pd.read_csv("all_catania_OD_all_vehicles.csv", delimiter=',')

## chose one ORIGIN and DESTINATION

# element = elements[25]
# index_elements = 25

# element = elements[103]
# index_elements = 103

# i = 418506072
# j = 3914961551

# elements.keys()
## make a list of all the elements keys ('element' is a dictionary object)
# element_list = list(elements.keys())
## reload 'element_list' as list
with open("D:/ENEA_CAS_WORK/Catania_RAFAEL/viasat_data/element_list_new.txt", "r") as file:
      element_list = eval(file.readline())

## read each 'element'
def func(arg):
    idx, index_elements = arg

    DELAY = 0
    DELAY_CELL = []
    print("CELL:", index_elements)
    gdf_elements = elements[index_elements]
    gdf_elements = pd.merge(gdf_elements, all_catania_OD[['u', 'v', 'ORIGIN', 'DESTINATION']], on=['u', 'v'], how='left')
    ## drop all rows with NA values
    gdf_elements = gdf_elements.dropna()
    # gdf_elements = gdf_elements.drop_duplicates(['ORIGIN', 'DESTINATION'])
    gdf_elements.reset_index(level=0, inplace=True)
    # O = list(gdf_elements.ORIGIN)
    O = list(gdf_elements.ORIGIN.unique())
    # D = list(gdf_elements.DESTINATION)
    D = list(gdf_elements.DESTINATION.unique())
    zipped_OD = zip(O, D)
    # loop ever each ORIGIN --> DESTINATION pair
    for (i, j) in zipped_OD:
        print(i,j)
        try:
            ## find shortest path based on the "cost" (time)
            ## NULL scenario
            init_shortest_OD_path_cost = nx.shortest_path(grafo, i, j, weight='VIASAT_cost')  # using cost (time)
            print("DO IT++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++")
            path_edges = list(zip(init_shortest_OD_path_cost,init_shortest_OD_path_cost[1:]))
            lr = nx.shortest_path_length(grafo, i,j, weight='VIASAT_cost') ## this is a time (seconds)
            lunghezza=[]
            if lr != 0:
                for l in path_edges:
                  lunghezza.append(grafo [l[0]] [l[1]] [0]['length'])  # get only the length for each arch between 2 path edges, [0] it the key = 0
                print("km:{0:.3f} h:{1:.3f} vm:{2:.0f}".format(sum(lunghezza)/1000, lr/3600, sum(lunghezza)/1000/lr*3600))  # units == km
                time_OD_NULL = lr/3600  # hours
                length_OD_NULL = sum(lunghezza)/1000  #km

                ## update MAP with paths
                df_nodes = pd.DataFrame(path_edges)
                df_nodes.columns = ['u', 'v']
                ## merge 'df_nodes' with 'gdf_edges'
                edges_shortest_route_VIASAT_cost = pd.merge(df_nodes, gdf_edges, on=['u', 'v'], how='left')
                edges_shortest_route_VIASAT_cost = gpd.GeoDataFrame(edges_shortest_route_VIASAT_cost)
                edges_shortest_route_VIASAT_cost.drop_duplicates(['u', 'v'], inplace=True)

                ## merge 'df_nodes' with 'TIME_EDGES' (sottorete)
                edges_matched_route_OD = pd.merge(df_nodes, TIME_EDGES, on=['u', 'v'], how='left')
                edges_matched_route_OD = gpd.GeoDataFrame(edges_matched_route_OD)
                edges_matched_route_OD.drop_duplicates(['u', 'v'], inplace=True)

                ## get the OD travel demand (counts/travelled time) (NULL SCENARIO, no penalty)
                ## https://aetransport.org/public/downloads/QIoR6/498-514ec4e5be18d.pdf
                if len(edges_matched_route_OD) > 0:
                    ## only use vehicle number (counts, MEAN from each EDGE)  and then divide by the time_OD_NULL....this is the TRAVEL_DEMAND_OD
                    counts = (pd.DataFrame(edges_matched_route_OD))['counts']
                    counts = counts.dropna()
                    ## Travel demand: divide by 4 (months), then by 30 (days), then by 16 (hours of higher traffic flow) (hours)
                    TRAVEL_DEMAND_OD = ((((counts.sum())/1)/30)/16)  # veichles/hour all over the path (trajectory) # if only for one month
                    # TRAVEL_DEMAND_OD = ((((counts.sum()) / 4) / 30) / 16)  # veichles/hour all over the path (trajectory) # if for 4 months



            ## for each edge (LINK) in the "element" assign a penalty time (disruption)
            # close a LINK (u,v pair) by adding sufficiently large penalty M (time in seconds ~ 5 hours = 18000 secs)
            for u, v, key, attr in grafo.edges(keys=True, data=True):
                attr['VIASAT_cost_penalty'] = attr.get("VIASAT_cost")
                zipped = zip(list(gdf_elements.u), list(gdf_elements.v))
                if (u, v) in zipped:
                    print(u,v)
                    print("gotta!=============================================================================================")
                    # break
                    attr['VIASAT_cost_penalty'] = float(attr['VIASAT_cost']) + penalty
                    print(attr['VIASAT_cost_penalty'])
                    grafo.add_edge(u, v, key, attr_dict=attr)
            # get shortest path again...but now with the PENALTY
            shortest_OD_path_VIASAT_penalty = nx.shortest_path(grafo, i, j,
                                                                    weight='VIASAT_cost_penalty')
            path_edges = list(zip(shortest_OD_path_VIASAT_penalty, shortest_OD_path_VIASAT_penalty[1:]))
            lr = nx.shortest_path_length(grafo, i, j, weight='VIASAT_cost_penalty')
            if lr !=0:
                lunghezza = []
                for l in path_edges:
                    lunghezza.append(grafo[l[0]][l[1]][0][
                                         'length'])  # get only the length for each arch between 2 path edges, [0] it the key = 0
                print("km:{0:.3f} h:{1:.3f} vm:{2:.0f}".format(sum(lunghezza) / 1000, lr / 3600,
                                                               sum(lunghezza) / 1000 / lr * 3600))  # units == km
                # total time from O--D with penalty
                time_OD_penalty = lr/3600
                length_OD_penalty = sum(lunghezza) / 1000  # km

                # update MAP with paths
                # add shortest path (by "VIASAT cost" to the my_map
                df_nodes = pd.DataFrame(path_edges)
                df_nodes.columns = ['u', 'v']

                ## merge 'df_nodes' with 'gdf_edges'
                edges_shortest_route_VIASAT_penalty = pd.merge(df_nodes, gdf_edges, on=['u', 'v'], how='left')
                edges_shortest_route_VIASAT_penalty = gpd.GeoDataFrame(edges_shortest_route_VIASAT_penalty)
                edges_shortest_route_VIASAT_penalty.drop_duplicates(['u', 'v'], inplace=True)

            ## calculate "closure impact" for each DESTINATION
            ## difference time between closure and normal conditions
            DT = time_OD_penalty - time_OD_NULL  # (hours)
            ## closure impact
            if DT < penalty / 3600:
                DELAY = DELAY + TRAVEL_DEMAND_OD * DT * ((penalty / 3600) - DT / 2)  # vehicles*hours

                #####---------------------------------------------###############################
                DELAY_OD = TRAVEL_DEMAND_OD * DT * ((penalty / 3600) - DT / 2)  # vehicles*hours
                df_OD = pd.DataFrame(edges_shortest_route_VIASAT_penalty, columns=['u', 'v'])
                df_OD['importance'] = DELAY_OD
                ### save into the DB
                #####---------------------------------------------###############################
                DELAY_CELL.append(DELAY)
                print("TOTAL DELAY:", DELAY)
                print("max(DELAY_CELL):", max(DELAY_CELL), "CELL:", index_elements)
            else:
                DELAY = DELAY + TRAVEL_DEMAND_OD * (((penalty / 3600) ** 2) / 2)  # vehicles*hours
                #####---------------------------------------------###############################
                DELAY_OD = TRAVEL_DEMAND_OD * (((penalty / 3600) ** 2) / 2)  # vehicles*hours
                df_OD = pd.DataFrame(edges_shortest_route_VIASAT_penalty, columns=['u', 'v'])
                df_OD['importance'] = DELAY_OD
                ### save into the DB
                #####---------------------------------------------###############################
                DELAY_CELL.append(DELAY)
                print("TOTAL DELAY:", DELAY)
                print("max(DELAY_CELL):", max(DELAY_CELL), "CELL:", index_elements)

            # restore initial travel time for each edge (LINK)....basically we need to remove the penalty time
            for u, v, key, attr in grafo.edges(keys=True, data=True):
                zipped = zip(list(gdf_elements.u), list(gdf_elements.v))
                if (u, v) in zipped:
                    # print(u,v)
                    # print("gotta!=============================================================================================")
                    attr['VIASAT_cost_penalty'] = float(attr['VIASAT_cost'])
                    grafo.add_edge(u, v, key, attr_dict=attr)

            with open("last_CELL_ID.txt", "w") as text_file:
                    text_file.write("last CELL ID: %s" % (index_elements))

            # ## for each (u,v) pair, create a new dataframe with field "importance"
            # U = list(gdf_elements.u)
            # V = list(gdf_elements.v)
            # df_vulnerability = list(zip(U, V))
            # df_vulnerability = pd.DataFrame(df_vulnerability, columns=['u', 'v'])
            # df_vulnerability['importance'] = DELAY
            # df_vulnerability['CELL'] = index_elements
            # df_vulnerability['importance'] = df_vulnerability.importance.astype('int')
            # df_vulnerability['importance'] = df_vulnerability['importance'].replace(np.nan, 0)
            # df_vulnerability = df_vulnerability[['u','v','CELL','importance']]
            # df_vulnerability.drop_duplicates(['u','v','CELL','importance'], inplace=True)
            #
            # ### Connect to a DB and populate the DB  ###
            # connection = engine.connect()
            # df_vulnerability.to_sql("vulnerability_2019", con=connection, schema="public",
            #                                    if_exists='append')
            # connection.close()

        except (nx.NodeNotFound, nx.exception.NetworkXNoPath):
            print('O-->D NodeNotFound', 'i:', i, 'j:', j)

    ## for each (u,v) pair, create a new dataframe with field "importance"
    U = list(gdf_elements.u)
    V = list(gdf_elements.v)
    df_vulnerability = list(zip(U, V))
    df_vulnerability = pd.DataFrame(df_vulnerability, columns=['u', 'v'])
    df_vulnerability['importance'] = DELAY
    df_vulnerability['CELL'] = index_elements
    df_vulnerability['importance'] = df_vulnerability.importance.astype('int')
    df_vulnerability['importance'] = df_vulnerability['importance'].replace(np.nan, 0)
    df_vulnerability = df_vulnerability[['u', 'v', 'CELL', 'importance']]
    df_vulnerability.drop_duplicates(['u', 'v', 'CELL', 'importance'], inplace=True)

    ### Connect to a DB and populate the DB  ###
    connection = engine.connect()
    # df_vulnerability.to_sql("vulnerability_pesanti_2019", con=connection, schema="public",
    #                         if_exists='append')
    # df_vulnerability.to_sql("vulnerability_all_FEBRUARY_2019", con=connection, schema="public",
    #                         if_exists='append')
    # df_vulnerability.to_sql("vulnerability_all_AUGUST_2019", con=connection, schema="public",
    #                        if_exists='append')
    df_vulnerability.to_sql("vulnerability_all_OD_AUGUST_2019", con=connection, schema="public",
                            if_exists='append')
    # df_vulnerability.to_sql("vulnerability_all_OD_FEBRUARY_2019_NEW", con=connection, schema="public",
    #                          if_exists='append')
    # df_vulnerability.to_sql("vulnerability_4seasons_2019_NEW", con=connection, schema="public",
    #                        if_exists='append')


################################################
##### run all script using multiprocessing #####
################################################

## check how many processer we have available:
# print("available processors:", mp.cpu_count())

if __name__ == '__main__':
    # pool = mp.Pool(processes=mp.cpu_count()) ## use all available processors
    pool = mp.Pool(processes=50)     ## use 60 processors
    print("++++++++++++++++ POOL +++++++++++++++++", pool)
    results = pool.map(func, [(idx, index_elements) for idx, index_elements in enumerate(element_list)])
    pool.close()
    pool.close()
    pool.join()
